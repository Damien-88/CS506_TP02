{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHLcriKWLRe4"
      },
      "source": [
        "## Intro to SciKit-Learn and its Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Learning Objectives:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvJBqX8_Bctk"
      },
      "source": [
        "Grasp Advanced Machine Learning Concepts: Understand core principles.\n",
        "\n",
        "Apply Dimensionality Reduction: Explain and utilize Principal Component Analysis (PCA) to reduce feature space while retaining variance.\n",
        "\n",
        "Master Feature Engineering and Selection: Employ techniques to enhance model performance.\n",
        "\n",
        "Perform Practical Dimensionality Reduction: Implement PCA using sklearn.decomposition for data visualization and efficiency.\n",
        "\n",
        "Execute Feature Selection: Apply SelectKBest and statistical scoring (e.g., ANOVA F-test) to identify key features.\n",
        "\n",
        "Comprehend Model Deployment Fundamentals: Describe saving, serializing (using joblib), and preparing models for production with Flask, FastAPI, or cloud platforms like AWS SageMaker.\n",
        "\n",
        "Recognize Real-World ML Applications: Identify machine learning uses across various industries (healthcare, finance, marketing, manufacturing, NLP).\n",
        "\n",
        "Gain Insight into Deep Learning: Differentiate classical ML from deep learning.\n",
        "\n",
        "Develop Basic Neural Networks: Build a feedforward network with Keras for classification.\n",
        "\n",
        "Code and Interpret ML Implementations:\n",
        "\n",
        "- Implement and visualize PCA.\n",
        "\n",
        "- Apply Scikit-learn's feature selection tools.\n",
        "\n",
        "- Serialize and deserialize models.\n",
        "\n",
        "- Build and train a neural network with TensorFlow/Keras.\n",
        "\n",
        "Develop Intuition for Model Complexity and Overfitting: Understand how dimensionality reduction and feature selection improve model generalization and reduce overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### What is SciKit-Learn:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIFJ83ZTBctl"
      },
      "source": [
        "Scikit-learn is a well-established open-source Python library widely utilized in the field of machine learning. Its architecture, built upon the foundational libraries of SciPy, NumPy, and Matplotlib, enables efficient predictive data analysis. Offering support for both supervised and unsupervised learning paradigms, Scikit-learn is recognized as an essential resource for data science practitioners."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Key Features:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Simple and consistent interface\n",
        "- Efficient tools for machine learning and statistical modeling\n",
        "- Built-in datasets for easy experimentation\n",
        "- Extensive documentation and community support\n",
        "- Integration with deep learning frameworks for advanced modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Due to its intuitive design and user-friendliness, Scikit-learn is frequently the initial choice for machine learning projects in Python. Its strong integration with fundamental data science libraries like Pandas and NumPy provides a comprehensive toolkit of algorithms covering classification, regression, clustering, model selection, and dimensionality reduction. This versatility allows it to be applied across various real-world domains, including healthcare analytics, fraud detection, recommendation systems, and predictive maintenance. Scikit-learn's fluid interaction with Pandas and NumPy, accepting both DataFrames and arrays as input and ensuring consistent utility across its components, streamlines the data science workflow, particularly for rapid prototyping and experimentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_JOISVgmn9v"
      },
      "source": [
        "## Environment Setup (Jupyter Notebook VS Extension)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Package Installation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSRYu62xUi3g"
      },
      "outputs": [],
      "source": [
        "# In the terminal enter the following\n",
        "pip install scikit-learn    # Core machine learning library\n",
        "pip install matplotlib      # Plotting and visualization\n",
        "pip install seaborn         # Statistical data visualization\n",
        "pip install joblib          # Model serialization (saving and loading trained models)\n",
        "pip install keras           # High-level deep learning API\n",
        "pip install tensorflow      # Deep learning library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All Library Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# At the top of your Python script, import the necessary libraries.\n",
        "# General\n",
        "import numpy as np                     # Numerical operations and arrays\n",
        "import pandas as pd                    # Data manipulation and analysis\n",
        "import matplotlib.pyplot as plt        # Creating plots and visualizations\n",
        "import seaborn as sns                  # Enhanced statistical visualizations\n",
        "\n",
        "# Scikit-learn Core Modules\n",
        "from sklearn.decomposition import PCA                           # Dimensionality reduction (e.g., Principal Component Analysis)\n",
        "from sklearn.feature_selection import SelectKBest, f_classif    # Feature selection using statistical tests\n",
        "from sklearn.model_selection import train_test_split            # Splitting data into training and testing sets\n",
        "from sklearn.metrics import (                                   # Evaluating model performance\n",
        "    confusion_matrix,                                           # Create a confusion matrix\n",
        "    ConfusionMatrixDisplay,                                     # Display confusion matrix\n",
        "    accuracy_score                                              # Calculate accuracy of classification models\n",
        ")\n",
        "from sklearn.linear_model import LinearRegression          # Creating and training linear regression models\n",
        "from sklearn.svm import SVC                                # Support Vector Classifier for classification tasks\n",
        "from sklearn.cluster import KMeans                         # Unsupervised clustering using K-Means\n",
        "from sklearn.mixture import GaussianMixture                # Probabilistic clustering with Gaussian Mixture Models\n",
        "from sklearn.preprocessing import StandardScaler           # Standardizing features\n",
        "from sklearn.pipeline import Pipeline                      # Creating streamlined workflows\n",
        "\n",
        "# Model Serialization\n",
        "import joblib                                               # Saving and loading trained models\n",
        "\n",
        "# Deep Learning with Keras / TensorFlow\n",
        "from keras.models import Sequential                         # To define a feedforward neural network model\n",
        "from keras.layers import Dense                              # To add fully connected layers to the neural network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daQreKXIUslr"
      },
      "source": [
        "## Basic Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scikit-learn provides a structured and coherent collection of modules for machine learning in Python. To effectively utilize this library, whether for introductory projects or developing robust production systems, a solid understanding of its core components is essential. The following sections outline these fundamental building blocks: datasets, preprocessing, model selection, and pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjnAk1xcU0yc"
      },
      "source": [
        "#### Datasets:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the realm of machine learning, datasets serve as the foundational collections of information used to train, validate, and evaluate models. Within Scikit-learn, these datasets come in two primary forms: readily available built-in datasets, ideal for rapid experimentation and learning, and external datasets, which can be imported from various sources such as files, databases, or online repositories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What Are Datasets Used For?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Practicing and experimenting with machine learning techniques\n",
        "- Testing model implementations\n",
        "- Demonstrating algorithms and workflows\n",
        "- Benchmarking models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Types of Datasets:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.\tToy Datasets (Built-in)\n",
        "These are small, clean, and pre-labeled datasets commonly used for education and testing.\n",
        "\n",
        "-\tExamples: iris, digits, wine, breast_cancer\n",
        "\n",
        "2.\tExternal Datasets\n",
        "These can be loaded using utility functions like fetch_openml() or from your local files using Pandas or Numpy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usecase Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use sklearn.datasets module to load datasets.\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Features and target\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Print feature names and first few rows\n",
        "print(\"Feature names:\", iris.feature_names)\n",
        "print(\"First row of features:\", X[0])\n",
        "print(\"Target names:\", iris.target_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example Output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature names: ['sepal length (cm)', 'sepal width (cm)', ...]\n",
        "\n",
        "First row of features: [5.1 3.5 1.4 0.2]\n",
        "\n",
        "Target names: ['setosa' 'versicolor' 'virginica']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preprocessing:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocessing is the process of transforming raw data into a clean and suitable format for machine learning models. Including cleaning data, scaling features, encoding categories, and generating new features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What Is Preprocessing Used For?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Making data compatible with machine learning algorithms\n",
        "- Improving model accuracy and generalization\n",
        "- Handling missing or categorical data\n",
        "- Creating richer representations of features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Types of Preprocessing Techniques:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.\tScaling & Normalization\n",
        "-\tAdjusts feature values to a common scale without distorting differences in the ranges of values.\n",
        "-\tTools: StandardScaler, MinMaxScaler\n",
        "\n",
        "2.\tEncoding Categorical Variables\n",
        "-\tConverts non-numeric categorical features into numeric formats.\n",
        "-\tTools: OneHotEncoder, OrdinalEncoder\n",
        "\n",
        "3.\tImputing Missing Values\n",
        "-\tFills in missing data using various strategies.\n",
        "-\tTool: SimpleImputer\n",
        "\n",
        "4.\tFeature Generation\n",
        "-\tCreates new features from existing ones, such as polynomial combinations.\n",
        "-\tTool: PolynomialFeatures\n",
        "\n",
        "5.\tData Output Formatting\n",
        "-\tUse set_output(transform=\"pandas\") to preserve DataFrame structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usecase Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFZ42Uq7UFDj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample dataset with missing values\n",
        "X = np.array([[1, 2], [3, np.nan], [5, 6]])\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "print(\"Imputed and scaled data:\\n\", X_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5ouUp1cU6pC"
      },
      "source": [
        "#### Model Selection:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Models are algorithms used to learn from data and make predictions. Scikit-learn supports a wide range of models for classification, regression, clustering, and more.\n",
        "\n",
        "Model selection is the process of:\n",
        "- Choosing the best model from a set of candidates\n",
        "- Optimizing its performance by tuning hyperparameters\n",
        "- Evaluating its generalizability using validation techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Examples of models:\n",
        "- Classification: LogisticRegression, RandomForestClassifier\n",
        "- Regression: LinearRegression, Ridge\n",
        "- Clustering: KMeans, DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What Is Model Selection Used For?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Identifying the most suitable model for your task\n",
        "- Preventing overfitting and underfitting\n",
        "- Fine-tuning model parameters for better accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Types of Model Selection Techniques:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.\tCross-Validation (cross_val_score)\n",
        "- Evaluates model performance by splitting data into training/validation sets multiple times.\n",
        "\n",
        "2.\tHyperparameter Tuning\n",
        "- Grid Search (GridSearchCV): Exhaustive search over parameter combinations.\n",
        "- Random Search (RandomizedSearchCV): Random combinations, faster for large spaces.\n",
        "\n",
        "3.\tPerformance Metrics\n",
        "- Classification: accuracy, precision, recall, f1_score\n",
        "- Regression: mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usecase Cross-validation with Logistic Regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avgr6GfiUh8t"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Create instance of LogisticRegression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Perform 3-fold cross-validation on model using scaled features and sample target variable [0, 1, 1]\n",
        "# Splits data into 3 parts, train on 2 parts and test on the 3rd, rotating each time\n",
        "scores = cross_val_score(model, X_scaled, [0, 1, 1], cv=3)\n",
        "\n",
        "# Print cross-validation scores for each fold\n",
        "print(\"Cross-validation scores:\", scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usecase Hyperparameter tuning with GridSearchCV:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define dictionary with hyperparameters to search\n",
        "# 'C' is regularization strength for Logistic Regression\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10]}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "# Performs cross-validated search over parameter grid using LogisticRegression\n",
        "# cv=5 means 5-fold cross-validation\n",
        "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
        "\n",
        "# Fit grid search on scaled features and target labels\n",
        "# [0, 1, 1] is target list\n",
        "grid_search.fit(X_scaled, [0, 1, 1])\n",
        "\n",
        "# Output best combination of parameters found\n",
        "print(\"Best parameters:\", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pipelines:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A pipeline in Scikit-learn represents an ordered series of data manipulation stages encapsulated within a single entity. This sequence comprises transformers, responsible for data preprocessing (like StandardScaler), and culminates in an estimator (LogisticRegression) that carries out the learning or prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What are Pipilines used for?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Structuring machine learning workflows\n",
        "- Making code cleaner and reproducible\n",
        "- Preventing data leakage during cross-validation\n",
        "- Facilitating hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Types of Pipelines:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.\tBasic Pipelines: Sequence of transformations and a final estimator.\n",
        "2.\tColumnTransformer Pipelines: Apply different preprocessing to different feature types.\n",
        "3.\tNested Pipelines: Combine multiple preprocessing and modeling steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usecase Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),         # Scale data\n",
        "    ('clf', LogisticRegression())         # Fit logistic regression\n",
        "])\n",
        "\n",
        "# Fit pipeline to data\n",
        "pipeline.fit(X_imputed, [0, 1, 1])\n",
        "\n",
        "# Make predictions\n",
        "preds = pipeline.predict(X_imputed)\n",
        "print(\"Predictions:\", preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Supervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Supervised learning is a key machine learning paradigm that trains models on labeled data. This means each input example has a corresponding correct output (label or target). The goal is for the model to learn the input-output mapping, enabling accurate predictions on novel, unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What is Supervised Learning Used For?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Predicting outcomes based on past data\n",
        "- Automating decision-making processes\n",
        "- Understanding relationships between variables\n",
        "- Solving practical problems like fraud detection, recommendation systems, speech recognition, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Types of Supervised Learning:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.\tRegression\n",
        "- Predicts continuous numerical values.\n",
        "- Examples: Predicting house prices, temperature forecasting, sales forecasting.\n",
        "\n",
        "2.\tClassification\n",
        "- Predicts discrete categories or classes.\n",
        "- Examples: Email spam detection (spam/not spam), image recognition (cat/dog), medical diagnosis (disease/no disease)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Examples and Usecases:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Supervised learning involves the following general steps:\n",
        "1.\tPrepare labeled data: Features (X) and target variable (y).\n",
        "2.\tSplit data: Usually into training and testing sets to evaluate performance.\n",
        "3.\tChoose a model: For regression or classification depending on the task.\n",
        "4.\tTrain the model: Fit it to the training data.\n",
        "5.\tMake predictions: Use the model to predict on unseen test data.\n",
        "6.\tEvaluate performance: Using appropriate metrics (e.g., MSE for regression, accuracy for classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Linear Regression Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=10)\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance using Mean Squared Error (MSE)\n",
        "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Linear Regression models the relationship between a dependent variable (target) and one or more independent variables (features) by fitting a linear equation.\n",
        "\n",
        "Use Cases:\n",
        "- Forecasting sales\n",
        "- Predicting housing prices\n",
        "- Analyzing financial trends"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Support Vector Machine (SVM) Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load wine dataset for classification\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Split dataset into train and test (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize SVM with linear kernel\n",
        "svm = SVC(kernel='linear')\n",
        "\n",
        "# Train classifier\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Support Vector Machines are powerful classification models that find the optimal boundary (hyperplane) separating classes. They work well in high-dimensional spaces and are robust to overfitting.\n",
        "\n",
        "Use Cases:\n",
        "- Text classification (spam detection)\n",
        "- Image recognition\n",
        "- Bioinformatics (cancer classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unsupervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unsupervised learning is a machine learning method where models learn patterns, structures, or groupings from unlabeled data. Differing from supervised learning, it doesn't use target outputs or labels; the algorithm's task is to find the inherent structure within the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What is Unsupervised Learning Used For?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Discovering hidden patterns or groupings in data\n",
        "- Customer segmentation in marketing\n",
        "- Image compression and segmentation\n",
        "- Recommender systems\n",
        "- Anomaly detection in fraud or fault detection\n",
        "- Data visualization and exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Types of Unsupervised Learning:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.\tClustering- Grouping data points into clusters based on similarity or distance metrics without predefined labels.\n",
        "- Examples: K-Means, Hierarchical Clustering, DBSCAN, Gaussian Mixture Models (GMM).\n",
        "\n",
        "2.\tDimensionality Reduction - Reducing the number of features while preserving important information, often for visualization or noise reduction.\n",
        "- Examples: Principal Component Analysis (PCA), t-SNE, UMAP.\n",
        "\n",
        "3.\tAnomaly Detection - Identifying outliers or unusual data points that do not conform to the expected pattern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Examples and Usecases:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unupervised learning involves the following general steps:\n",
        "1.\tPrepare input data: Features (X) only, no target labels needed.\n",
        "2.\tChoose an unsupervised algorithm: Clustering or dimensionality reduction.\n",
        "3.\tFit the model: Learn the structure from the data.\n",
        "4.\tAnalyze output: Cluster labels, transformed components, or anomaly scores.\n",
        "5.\tVisualize or interpret results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "K-Means Clustering Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "X, _ = load_iris(return_X_y=True)\n",
        "\n",
        "# Initialize KMeans with 3 clusters\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "\n",
        "# Fit and predict cluster labels\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Visualize clusters on first two features\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.title(\"K-Means Clustering\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "K-Means is a simple and popular clustering algorithm that partitions data into k distinct clusters by minimizing the within-cluster variance.\n",
        "\n",
        "Use Cases:\n",
        "- Customer segmentation\n",
        "- Market research\n",
        "- Image compression\n",
        "- Pattern recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gaussian Mixture Model (GMM) Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Initialize GMM with 3 components\n",
        "gmm = GaussianMixture(n_components=3, random_state=42)\n",
        "\n",
        "# Fit and predict cluster memberships\n",
        "gmm_labels = gmm.fit_predict(X)\n",
        "\n",
        "# Visualize GMM clusters on first two features\n",
        "plt.scatter(X[:, 0], X[:, 1], c=gmm_labels, cmap='coolwarm')\n",
        "plt.title(\"Gaussian Mixture Model Clustering\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GMM models the data as a mixture of several Gaussian distributions, providing a probabilistic clustering that can capture more complex cluster shapes than K-Means.\n",
        "\n",
        "Use Cases:\n",
        "- Voice recognition\n",
        "- Image classification\n",
        "- Anomaly detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualization techniques in machine learning use graphical representations of data, model outputs, and evaluation metrics to improve understanding of data structure, model behavior, and prediction quality. These visual tools aid in interpreting complex models, identifying patterns or issues, and clearly communicating results.\n",
        "\n",
        "Scikit-learn integrates with libraries like Matplotlib and provides utilities for visualization (e.g., ConfusionMatrixDisplay). The typical workflow:\n",
        "1.\tTrain and test a model\n",
        "2.\tObtain predictions or relevant metrics\n",
        "3.\tGenerate plots to interpret results\n",
        "4.\tAnalyze and iterate on the model or data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What Are Visualization Techniques Used For?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Diagnosing model strengths and weaknesses\n",
        "- Understanding model decision-making\n",
        "- Exploring data distribution and separability\n",
        "- Communicating results to stakeholders\n",
        "- Guiding feature engineering and model tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Types of Visualization Techniques in Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.\tConfusion Matrix\n",
        "Shows the performance of classification models by summarizing true vs. predicted labels. Helps diagnose types of errors such as false positives and false negatives.\n",
        "\n",
        "2.\tDecision Boundary Plots\n",
        "Visualize the regions in feature space where a classifier assigns different classes. Useful for understanding how a model separates different classes.\n",
        "\n",
        "3.\tFeature Importance / Coefficients Plots\n",
        "Show which features contribute most to the model's decisions (common in tree-based or linear models).\n",
        "\n",
        "4.\tLearning Curves\n",
        "Show model performance as a function of training set size, helping diagnose underfitting or overfitting.\n",
        "\n",
        "5.\tResidual Plots\n",
        "Common in regression, they visualize errors to check model assumptions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usecases and Examples:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confusion Matrix Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Load data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train classifier\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Display confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confusion matrices are especially helpful for classification models to understand types of errors being made (false positives vs. false negatives)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decision Boundary Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load data (only first two features for visualization)\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_two = X[:, :2]\n",
        "\n",
        "# Train SVM classifier\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_two, y)\n",
        "\n",
        "# Create meshgrid over feature space\n",
        "x_min, x_max = X_two[:, 0].min() - 1, X_two[:, 0].max() + 1\n",
        "y_min, y_max = X_two[:, 1].min() - 1, X_two[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                     np.linspace(y_min, y_max, 100))\n",
        "\n",
        "# Predict class for each point in meshgrid\n",
        "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot decision boundary and data points\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
        "plt.scatter(X_two[:, 0], X_two[:, 1], c=y, edgecolors='k', cmap='coolwarm')\n",
        "plt.title(\"Decision Boundary (SVM)\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decision boundaries show how a classifier divides the feature space. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Topics and Real-World Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you move beyond introductory machine learning, you'll encounter advanced techniques and workflows designed to improve model performance, interpretability, and deployment readiness. Significantly, these topics facilitate the transition from experimental settings to practical real-world applications. Advanced areas such as dimensionality reduction, feature selection, production model deployment, and deep learning framework integration become crucial when tackling complex data challenges or large-scale deployments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Examples of Some Advanced Topics:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Dimensionality Reduction with PCA\n",
        "\n",
        "Dimensionality Reduction techniques reduce the number of input features while preserving as much information as possible. PCA (Principal Component Analysis) is a popular linear method that projects data onto principal components capturing the greatest variance.\n",
        "\n",
        "Used for:\n",
        "- Visualizing high-dimensional data in 2D or 3D\n",
        "- Speeding up training by reducing features\n",
        "- Reducing overfitting by eliminating noise and redundancy\n",
        "\n",
        "2. Feature Engineering and Selection\n",
        "\n",
        "Feature engineering involves creating new features or selecting the most important existing ones to improve model accuracy and interpretability. Feature Selection specifically removes irrelevant or redundant features using statistical tests or model-based importance scores.\n",
        "\n",
        "Used for:\n",
        "- Improving model performance\n",
        "- Reducing training time\n",
        "- Enhancing interpretability\n",
        "\n",
        "3. Model Deployment\n",
        "\n",
        "Model deployment involves saving a trained model and integrating it into production systems to make predictions on new data in real time or batch mode. Models can be deployed using frameworks like Flask or FastAPI for web APIs, or cloud platforms like AWS SageMaker or Google Cloud AI Platform.\n",
        "\n",
        "Used for:\n",
        "- Bringing models from development to production\n",
        "- Serving models via APIs or cloud services\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Real-World Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Supervised Learning.\n",
        "- Used in the medical field, hospitals, diagnostic AI systems\n",
        "- Uses logistic Regression, Support Vector Machines (SVM)\n",
        "- Used to classify tumors (e.g., benign vs. malignant) based on medical imaging and test results.\n",
        "\n",
        "Unsupervised Learning.\n",
        "- Used in marketing teams in retail and e-commerce\n",
        "- Uses K-Means Clustering\n",
        "- Used to Group customers by behavior patterns to improve ad targeting and product recommendations.\n",
        "\n",
        "Visualization Techniques\n",
        "- Used in Machine learning dashboards, MLOps tools\n",
        "- Uses Decision Boundary Plots\n",
        "- Used to show how a model separates different classes in feature space, often for SVM or logistic regression.\n",
        "\n",
        "Preprocessing\n",
        "- Used in banking systems, fintech platforms\n",
        "- Uses StandardScaler, OneHotEncoder, SimpleImputer\n",
        "- Used to normalize numerical inputs, handle missing values, and encode categorical data before modeling.\n",
        "\n",
        "Model Selection\n",
        "- Used in E-commerce\n",
        "- Uses Cross-Validation, GridSearchCV\n",
        "- Used to compare multiple algorithms and hyperparameters to find the best performing model.\n",
        "\n",
        "Pipelines\n",
        "- Used in end-to-end ML projects\n",
        "- Uses Pipeline to combine preprocessing and model fitting\n",
        "- Used to Automatically scale data, train models, and prevent data leakage in one reusable workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chollet, F. (2018). Deep learning with Python . Manning Publications.\n",
        "\n",
        "Jolliffe, I. T., & Cadima, J. (2016). Principal component analysis: A review and recent developments. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150202. https://doi.org/10.1098/rsta.2015.0202\n",
        "\n",
        "Scikit-learn contributors. (2023). sklearn.decomposition.PCA — scikit-learn 1.2.2 documentation. https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html \n",
        "\n",
        "Guyon, I., & Elisseeff, A. (2003). An introduction to variable and feature selection. Journal of Machine Learning Research, 3, 1157–1182. http://jmlr.org/papers/v3/guyon03a.html\n",
        "\n",
        "Scikit-learn contributors. (2023). sklearn.feature_selection.SelectKBest — scikit-learn 1.2.2 documentation. https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html \n",
        "\n",
        "Sculley, D., Holt, G., Golovin, D., Davydov, E., Phillips, T., Ebner, D., Chaudhary, V., Young, M., Crespo, J.-F., & Dennison, D. (2015). Hidden technical debt in machine learning systems. In Advances in neural information processing systems (Vol. 28). https://papers.nips.cc/paper/2015/file/86df7e3c2697d3fce7e7f3a5a5933e42-Paper.pdf \n",
        "\n",
        "Python Software Foundation. (2023). joblib: Running Python functions as pipeline jobs. https://joblib.readthedocs.io/en/latest/\n",
        "\n",
        "Jordan, M. I., & Mitchell, T. M. (2015). Machine learning: Trends, perspectives, and prospects. Science, 349(6245), 255–260. https://doi.org/10.1126/science.aaa8415\n",
        "\n",
        "Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT Press. https://www.deeplearningbook.org/\n",
        "\n",
        "Keras Documentation. (2023). Keras: The Python deep learning API. https://keras.io/api/models/sequential/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Code Source References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scikit-learn:\n",
        "\n",
        "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, É. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825–2830. http://jmlr.org/papers/v12/pedregosa11a.html\n",
        "\n",
        "\n",
        "Matplotlib:\n",
        "\n",
        "Hunter, J. D. (2007). Matplotlib: A 2D graphics environment. Computing in Science & Engineering, 9(3), 90–95. https://doi.org/10.1109/MCSE.2007.55 \n",
        "\n",
        "\n",
        "Keras/TensorFlow:\n",
        "\n",
        "Abadi, M., et al. (2016). TensorFlow: A system for large-scale machine learning. 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI ’16), 265–283. https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JndnmDMp66FL",
        "YHIWvc9Ms-Ll",
        "TJffr5_Jwqvd"
      ],
      "name": "intro_to_pandas.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
